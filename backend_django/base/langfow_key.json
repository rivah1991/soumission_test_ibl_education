{"id":"224aaba0-b9f7-4e61-a690-dc4ae674d868","data":{"nodes":[{"data":{"description":"Get chat inputs from the Playground.","display_name":"Chat Input","id":"ChatInput-Fwj5F","node":{"base_classes":["Message"],"beta":false,"conditional_paths":[],"custom_fields":{},"description":"Get chat inputs from the Playground.","display_name":"Chat Input","documentation":"","edited":false,"field_order":["input_value","store_message","sender","sender_name","session_id","files"],"frozen":false,"icon":"ChatInput","output_types":[],"outputs":[{"cache":true,"display_name":"Message","method":"message_response","name":"message","selected":"Message","types":["Message"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_NAME_USER\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n"},"files":{"advanced":true,"display_name":"Files","dynamic":false,"fileTypes":["txt","md","mdx","csv","json","yaml","yml","xml","html","htm","pdf","docx","py","sh","sql","js","ts","tsx","jpg","jpeg","png","bmp","image"],"file_path":"","info":"Files to be sent with the message.","list":true,"name":"files","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"file","value":""},"input_value":{"advanced":false,"display_name":"Text","dynamic":false,"info":"Message to be passed as input.","input_types":["Message"],"list":false,"load_from_db":false,"multiline":true,"name":"input_value","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"hi where is Paris?"},"sender":{"advanced":true,"display_name":"Sender Type","dynamic":false,"info":"Type of sender.","name":"sender","options":["Machine","User"],"placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"str","value":"User"},"sender_name":{"advanced":true,"display_name":"Sender Name","dynamic":false,"info":"Name of the sender.","input_types":["Message"],"list":false,"load_from_db":false,"name":"sender_name","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"User"},"session_id":{"advanced":true,"display_name":"Session ID","dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","input_types":["Message"],"list":false,"load_from_db":false,"name":"session_id","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"should_store_message":{"_input_type":"BoolInput","advanced":true,"display_name":"Store Messages","dynamic":false,"info":"Store the message in the history.","list":false,"name":"should_store_message","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":true}},"lf_version":"1.0.18"},"type":"ChatInput"},"dragging":false,"height":298,"id":"ChatInput-Fwj5F","position":{"x":-401.87576299753215,"y":1137.8100043461504},"positionAbsolute":{"x":-401.87576299753215,"y":1137.8100043461504},"selected":false,"type":"genericNode","width":384},{"data":{"description":"Create a prompt template with dynamic variables.","display_name":"Prompt","id":"Prompt-YCXBm","node":{"template":{"_type":"Component","code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"},"template":{"advanced":false,"display_name":"Template","dynamic":false,"info":"","list":false,"load_from_db":false,"name":"template","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"type":"prompt","value":"Answer the user in a joyful and enthusiastic tone, like a pirate of the seas.\n\nUser: {user_input}\n\nPirate's response:"},"user_input":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"user_input","display_name":"user_input","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["user_input"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false,"lf_version":"1.0.18"},"type":"Prompt"},"dragging":false,"height":412,"id":"Prompt-YCXBm","position":{"x":60.441484540701595,"y":827.4777177176301},"positionAbsolute":{"x":60.441484540701595,"y":827.4777177176301},"selected":false,"type":"genericNode","width":384},{"data":{"description":"Display a chat message in the Playground.","display_name":"Chat Output","id":"ChatOutput-sqFjS","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"data_template":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"data_template","value":"{text}","display_name":"Data Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as output.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"sender","value":"Machine","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str","_input_type":"DropdownInput"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"sender_name","value":"AI","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"session_id","value":"","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"should_store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"should_store_message","value":true,"display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Display a chat message in the Playground.","icon":"ChatOutput","base_classes":["Message"],"display_name":"Chat Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","should_store_message","sender","sender_name","session_id","data_template"],"beta":false,"edited":false,"lf_version":"1.0.18"},"type":"ChatOutput"},"dragging":false,"height":296,"id":"ChatOutput-sqFjS","position":{"x":1117.0017683621554,"y":1197.35458165956},"positionAbsolute":{"x":1117.0017683621554,"y":1197.35458165956},"selected":false,"type":"genericNode","width":384},{"id":"HuggingFaceModel-TBSYl","type":"genericNode","position":{"x":575.7818586605525,"y":749.5557292047956},"data":{"type":"HuggingFaceModel","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from tenacity import retry, stop_after_attempt, wait_fixed\nfrom langchain_community.llms.huggingface_endpoint import HuggingFaceEndpoint\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.io import DictInput, DropdownInput, SecretStrInput, StrInput, IntInput\n\n\nclass HuggingFaceEndpointsComponent(LCModelComponent):\n    display_name: str = \"HuggingFace\"\n    description: str = \"Generate text using Hugging Face Inference APIs.\"\n    icon = \"HuggingFace\"\n    name = \"HuggingFaceModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        StrInput(\n            name=\"model_id\",\n            display_name=\"Model ID\",\n            value=\"openai-community/gpt2\",\n        ),\n        DropdownInput(\n            name=\"task\",\n            display_name=\"Task\",\n            options=[\"text2text-generation\", \"text-generation\", \"summarization\", \"translation\"],\n            value=\"text-generation\",\n        ),\n        SecretStrInput(name=\"huggingfacehub_api_token\", display_name=\"API Token\", password=True),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Keyword Arguments\", advanced=True),\n        IntInput(name=\"retry_attempts\", display_name=\"Retry Attempts\", value=1, advanced=True),\n    ]\n\n    def create_huggingface_endpoint(\n        self, model_id: str, task: str, huggingfacehub_api_token: str, model_kwargs: dict\n    ) -> HuggingFaceEndpoint:\n        retry_attempts = self.retry_attempts  # Access the retry attempts input\n        endpoint_url = f\"https://api-inference.huggingface.co/models/{model_id}\"\n\n        @retry(stop=stop_after_attempt(retry_attempts), wait=wait_fixed(2))\n        def _attempt_create():\n            return HuggingFaceEndpoint(\n                endpoint_url=endpoint_url,\n                task=task,\n                huggingfacehub_api_token=huggingfacehub_api_token,\n                model_kwargs=model_kwargs,\n            )\n\n        return _attempt_create()\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        model_id = self.model_id\n        task = self.task\n        huggingfacehub_api_token = self.huggingfacehub_api_token\n        model_kwargs = self.model_kwargs or {}\n\n        try:\n            llm = self.create_huggingface_endpoint(model_id, task, huggingfacehub_api_token, model_kwargs)\n        except Exception as e:\n            raise ValueError(\"Could not connect to HuggingFace Endpoints API.\") from e\n\n        return llm\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"huggingfacehub_api_token":{"load_from_db":true,"required":false,"placeholder":"","show":true,"name":"huggingfacehub_api_token","value":"","display_name":"API Token","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"model_id":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"model_id","value":"EleutherAI/gpt-neo-2.7B","display_name":"Model ID","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"model_kwargs","value":{},"display_name":"Model Keyword Arguments","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict","_input_type":"DictInput"},"retry_attempts":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"retry_attempts","value":1,"display_name":"Retry Attempts","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"task":{"trace_as_metadata":true,"options":["text2text-generation","text-generation","summarization","translation"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"task","value":"text-generation","display_name":"Task","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"}},"description":"Generate text using Hugging Face Inference APIs.","icon":"HuggingFace","base_classes":["LanguageModel","Message"],"display_name":"HuggingFace","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","system_message","stream","model_id","task","huggingfacehub_api_token","model_kwargs","retry_attempts"],"beta":false,"edited":false,"lf_version":"1.0.18"},"id":"HuggingFaceModel-TBSYl"},"selected":false,"width":384,"height":629,"positionAbsolute":{"x":575.7818586605525,"y":749.5557292047956},"dragging":false}],"edges":[{"source":"Prompt-YCXBm","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-YCXBmœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"HuggingFaceModel-TBSYl","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œHuggingFaceModel-TBSYlœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"HuggingFaceModel-TBSYl","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-YCXBm","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-YCXBm{œdataTypeœ:œPromptœ,œidœ:œPrompt-YCXBmœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-HuggingFaceModel-TBSYl{œfieldNameœ:œinput_valueœ,œidœ:œHuggingFaceModel-TBSYlœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":"","selected":false},{"source":"ChatInput-Fwj5F","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-Fwj5Fœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-YCXBm","targetHandle":"{œfieldNameœ:œuser_inputœ,œidœ:œPrompt-YCXBmœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"user_input","id":"Prompt-YCXBm","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-Fwj5F","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-Fwj5F{œdataTypeœ:œChatInputœ,œidœ:œChatInput-Fwj5Fœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-YCXBm{œfieldNameœ:œuser_inputœ,œidœ:œPrompt-YCXBmœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","selected":false,"className":""},{"source":"HuggingFaceModel-TBSYl","sourceHandle":"{œdataTypeœ:œHuggingFaceModelœ,œidœ:œHuggingFaceModel-TBSYlœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"ChatOutput-sqFjS","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-sqFjSœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-sqFjS","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"HuggingFaceModel","id":"HuggingFaceModel-TBSYl","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-HuggingFaceModel-TBSYl{œdataTypeœ:œHuggingFaceModelœ,œidœ:œHuggingFaceModel-TBSYlœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-sqFjS{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-sqFjSœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"}],"viewport":{"x":-96.29157896969252,"y":-567.9927616462469,"zoom":0.6389404122206768}},"description":"This flow will get you experimenting with the basics of the UI, the Chat and the Prompt component. \n\nTry changing the Template in it to see how the model behaves. \nYou can change it to this and a Text Input into the `type_of_person` variable : \"Answer the user as if you were a pirate.\n\nUser: {user_input}\n\nAnswer: \" ","name":"langfow_key","last_tested_version":"1.0.18","endpoint_name":null,"is_component":false}